\documentclass[11pt]{article}
\usepackage{geometry}
\geometry{letterpaper}
\geometry{margin=0.5in}


\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}
% credit: http://tex.stackexchange.com/questions/68091/how-do-i-add-syntax-coloring-to-my-c-source-code-in-beamer
% https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
\lstset{language=C,
        basicstyle=\ttfamily,
        keywordstyle=\color{blue}\ttfamily,
        stringstyle=\color{red}\ttfamily,
        commentstyle=\color{green}\ttfamily,
        morecomment=[l][\color{magenta}]{\#}
}

\title{\textbf{CS 5220 Project 1 Final Report}}
\author{Stephen McDowell (sjm324)}
\date{\textbf{September 2015}}

\usepackage{natbib}
\usepackage[miktex]{gnuplottex}
\usepackage{graphicx}
\usepackage{color}
\usepackage{pdfpages}

\usepackage{enumerate}

\newcommand{\tab}{\hspace*{2em}}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\sub}{\textsubscript}
\newcommand{\Depth}{2}
\newcommand{\Height}{2}
\newcommand{\Width}{2}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{relsize}
\usepackage{pdfpages}
\usepackage{hyperref}

\def\wl{\par \vspace{\baselineskip}}

\makeatletter
\renewcommand{\maketitle}{\bgroup\setlength{\parindent}{0pt}
\begin{flushleft}
  {\Large \textsc{\@title}}\newline
  \textsc{\@author}
  \rule{\textwidth}{1pt}
\end{flushleft}\egroup
}
\makeatother

\usepackage{fancyhdr}
\pagestyle{fancy}
\rhead{\textsc{Group 003} -- rmc298, sjm324 -- \textsc{Page} \thepage}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{0.5in}

\title{CS 5220: Project 3 Initial Report}
\author{Group 003: Robert Chiodi (rmc298), Stephen McDowell (sjm324)}

\begin{document}
\thispagestyle{empty}
\maketitle

\section{Introduction}
For the third assignment, we employ techniques we have learned in previous assignments as well as explore the capabilities of the Message Passing Interface (MPI) in the context of the \href{https://en.wikipedia.org/wiki/Floyd\%E2\%80\%93Warshall_algorithm}{\textcolor{blue}{\underline{Floyd-Warshall}}} algorithm for computing all pairwise shortest paths.  As described in the assignment description, this assignment is very similar to the matrix multiplication algorithm we have studied previously in terms of overall problem setup, and only differs in the actual computations being performed.\\

\noindent We begin by focusing our efforts on using MPI to allow computation across multiple nodes on the \emph{totient} cluster, and then discuss optimizations of the algorithm to speedup the computation.  We end with a discussion of offloading the computation to the Xeon Phi's, and a summary of our tactics.\\

\noindent It is worth mentioning at this point that the assignment asked us to implement a parallel MPI algorithm, and then choose between optimizing the given OpenMP implementation and our own MPI implementation.  We choose to do both for two reasons:

\begin{enumerate}[1.]
  \item The optimizations for the OpenMP implementation are very much the same as the MPI implementation in that the algorithm being executed is the same -- the only difference being how threads are delegated work, and
  \item Recent trends in high performance computing fall more along the lines of ``first, use MPI to coordinate between multiple nodes, then use OpenMP on each node to further parallelize the work.''
\end{enumerate}

\section{Optimizations}

\begin{equation}
\mathrm{Strong \; Scaling} = \frac{t_{\mathrm{serial}}}{t_{\mathrm{parallel}}}
\label{strongscale}
\end{equation}

\begin{equation}
\mathrm{Strong \; Scaling \; Efficiency} = \frac{t_{\mathrm{serial}}}{t_{\mathrm{parallel}}} \frac{1}{p}
\label{normss}
\end{equation}

\begin{equation}
\mathrm{Weak \; Scaling} = \frac{t_{\mathrm{serial}}(n(p))}{t_{\mathrm{parallel}}(n(p),p)}
\label{ws}
\end{equation}

\section{Message Passing Interface (MPI)}
The method of MPI parallelization was chosen for its ease of implementation and decent performance. To start, the Floyd-Warshall pathing algorithm, originally parallelized using OpenMP, was changed in order to make use of MPI. First, the parallelization and delegation of work was moved outside of the square function to the main function, in order to eliminate multiple parallel initialization costs. From this point, the pathing matrix is split into $\mathtt{npx}\times \mathtt{npy}$ subdomains, where \texttt{npx} and \texttt{npy} are the number of processors used to divide the matrix in the horizontal and vertical directions, respectively. \texttt{MPI\_CART} commands are used to organize these threads in an optimal orientation and provide coordinates for the responsibilities of each processor, which are then used to determine the starting and ending indices of each threads subdomain in relation to the global pathing matrix. At this point, each thread has its own copy of the entire pathing matrix and calls the \texttt{shortest\_paths} function. \\

\noindent The functions \texttt{infinitize} and \texttt{deinfinitize}, as well as the diagonal zeroing loop, were not parallelized due to their relatively little computational expense. In doing so, the associated parallel communication costs were avoided. The values of \texttt{l} are then copied into \texttt{lnew} using \texttt{MPI\_ALLREDUCE} with the \texttt{MPI\_MIN} operator. In this way, consistency between processors is ensured. It would also be easily possible to parallelize \texttt{infinitize} and \texttt{deinfinitize} with this method of communication, if the problem size became large enough to warrant the additional communication costs associated with it. \\

\noindent In the main loop, the \texttt{square} function is called by each thread, where each thread determines the shortest paths for its subdomain. Once each thread is finished inside square, \texttt{MPI\_ALLREDUCE} is once again used with the \texttt{MPI\_MIN} operator to determine if all processors returned a value of 1, indicating all shortest paths have been found. The array \texttt{lnew} is then communicated and stored in \texttt{l}, once again using \texttt{MPI\_ALLREDUCE} with the \texttt{MPI\_MIN} operator. \\

While this is not the most efficient MPI implementation, we believe it to be sufficient to show that shared memory OpenMP will be significantly faster than MPI for problem sizes that fit on one node. This is due to the nature of the pathing algorithm, where every point to the left, right, top, and bottom of the element of interest needs to be checked to determine if it provides a shorter path, making this more efficient with shared memory. If the problem sizes driven by an application scaled greatly so that they could no longer fit on one node, the MPI implementation could be rewritten to provide some marginal gains in performance and noticeable gains in memory efficiency, enabling the algorithm to be run on very large problem sizes. \\

\noindent Due to the ability of MPI to utilize several nodes, we believe a heterogeneous parallelization strategy would be ideal, where MPI is used for inter-node communications and OpenMP is otherwise used to compute on the Xeon Phi cores. At this time, however, we have been unable to run on multiple nodes of Totient using MPI, preventing us from implementing this.

\subsection{MPI Scaling Study}
In order to understand the performance of the MPI-parallelized shortest paths algorithm, strong and weak scaling studies were run. For the strong scaling, six simulations were run for a problem size of $4000 \times 4000$ for four separate probabilities, 0.025, 0.05, 0.1, 0.3. The results are plotted in Figure~\ref{mpi_ss}.

\begin{figure}[h]
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			#set key at 15.8,41
			#set size 1.0,0.5              
			unset log                          
			unset label                          
			#set xtic auto offset 0,-0.5 font ",10"                     
			#set ytic 25 font ",10" 
			#Set Info
			set xlabel "Number of Threads"
			#set xlabel offset 0,-0.5
			set ylabel "Scaling Efficiency"
			#set ylabel offset -1.25,0
			#set xr [0.0:16.0]
			set yr [0.0:1.0]
			plot "./benchmarking/mpi/strong/p025strong.txt" u 1:(490.5691/$2/$1) t 'p = 0.025' w linespoints, \
			"./benchmarking/mpi/strong/p05strong.txt" u 1:(538.9763960838318/$2/$1) t 'p = 0.050' w lp, \
			"./benchmarking/mpi/strong/p10strong.txt" u 1:(325.8776059150/$2/$1) t 'p = 0.100' w lp, \
			"./benchmarking/mpi/strong/p30strong.txt" u 1:(325.54634/$2/$1) t 'p = 0.300' w lp
		\end{gnuplot}
		\caption{Strong scaling for MPI parallelized path algorithm with $\mathtt{n} = 4000$.}
		\label{mpi_ss}
	\end{center}
\end{figure}
\noindent A couple comments can be made about the parallel efficiency shown in Figure~\ref{mpi_ss}. First, there is a stark decrease in scaling efficiency between 8 and 16 threads. This is most likely due to an increase in communication costs, as running the program with 8 threads only requires communication on the same Xeon chip, while 16 threads spans two Xeon chips. For all probabilities presented here, this actually leads to the program completing quicker when run with only 8 processors as opposed to 16. A second comment that can be made is that scaling efficiency appears to be independent of the probability parameter. \\

\noindent Typically, weak scaling is used to view the communication costs associated with the parallelizing a program since the algorithmic work should remain constant per processor during the study. This should allow us to see if communications become a dominant source of time in our program, requiring us to rethink our MPI parallelization strategy. The parameters for our weak scaling study can be seen in Table~\ref{mpi_ws_tab}, where each parameter configuration was one again run for probabilities of 0.025, 0.05, 0.1, and 0.3. The results of the study are shown in Figure~\ref{mpi_ws}

\begin{figure}[h]
	\begin{center}
	\begin{tabular}{|c| c|}
		\hline
		\# of Threads & n \\ \hline
		1 & 1000 \\ \hline
		2 & 1414 \\ \hline
		4 & 2000 \\ \hline
		8 & 2828 \\ \hline
		16 & 4000 \\ \hline
		24 & 4899 \\ \hline
	\end{tabular}
		\end{center}
		\caption{Table of weak scaling parameters for number of threads and the squart root of the problem size ($n$)}
		\label{mpi_ws_tab}
\end{figure}

\begin{figure}[h]
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			#set key at 15.8,41
			#set size 1.0,0.5              
			unset log                          
			unset label                          
			#set xtic auto offset 0,-0.5 font ",10"                     
			#set ytic 25 font ",10" 
			#Set Info
			set xlabel "Number of Threads"
			#set xlabel offset 0,-0.5
			set ylabel "Weak Scaling"
			#set ylabel offset -1.25,0
			#set xr [0.0:16.0]
			set yr [0.0:1.2]
			plot "./benchmarking/mpi/weak/p025weak.txt" u 1:(2.49040075/$2) t 'p = 0.025' w linespoints, \
			"./benchmarking/mpi/weak/p05weak.txt" u 1:(2.4700660705/$2) t 'p = 0.050' w lp, \
			"./benchmarking/mpi/weak/p10weak.txt" u 1:(2.49511981/$2) t 'p = 0.100' w lp, \
			"./benchmarking/mpi/weak/p30weak.txt" u 1:(1.6716897/$2) t 'p = 0.300' w lp
		\end{gnuplot}
		\caption{Weak scaling for MPI parallelized path algorithm with program parameters given in Table~\ref{mpi_ws_tab}.}
		\label{mpi_ws}
	\end{center}
\end{figure}
\noindent From the weak scaling, it can once again be seen that large communication costs are associated with using threads that span two Xeon chips by comparing the weak scaling at 8 threads and 16 threads. The poor performance indicates that if we desire to use the MPI parallelization as anything more than a message passer for OpenMP thread teams running on Xeon Phi's significant effort should be spent on improving parallelization through reducing the communication frequency and the size of data communicated. The higher than one weak scaling for two threads with a probability of 0.1 is due to the fact that there is 302 less nodes in the system per processor. This is most likely not an issue for the other parameters due to a lower amount of times entered the reassignment loop inside the \texttt{square} function.


\section{Offloading to the Phi's}

\section{Summary}


\end{document}
