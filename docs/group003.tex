\documentclass[11pt]{article}
\usepackage{geometry}
\geometry{letterpaper}
\geometry{margin=0.5in}


\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}
% credit: http://tex.stackexchange.com/questions/68091/how-do-i-add-syntax-coloring-to-my-c-source-code-in-beamer
% https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
\lstset{language=C,
        basicstyle=\ttfamily,
        keywordstyle=\color{blue}\ttfamily,
        stringstyle=\color{red}\ttfamily,
        commentstyle=\color{green}\ttfamily,
        morecomment=[l][\color{magenta}]{\#}
}

\title{\textbf{CS 5220 Project 1 Final Report}}
\author{Stephen McDowell (sjm324)}
\date{\textbf{September 2015}}

\usepackage{natbib}
\usepackage[miktex]{gnuplottex}
\usepackage{graphicx}
\usepackage{color}
\usepackage{pdfpages}

\usepackage{enumerate}

\newcommand{\tab}{\hspace*{2em}}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\sub}{\textsubscript}
\newcommand{\Depth}{2}
\newcommand{\Height}{2}
\newcommand{\Width}{2}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{relsize}
\usepackage{pdfpages}
\usepackage{hyperref}

\def\wl{\par \vspace{\baselineskip}}

\makeatletter
\renewcommand{\maketitle}{\bgroup\setlength{\parindent}{0pt}
\begin{flushleft}
  {\Large \textsc{\@title}}\newline
  \textsc{\@author}
  \rule{\textwidth}{1pt}
\end{flushleft}\egroup
}
\makeatother

\usepackage{fancyhdr}
\pagestyle{fancy}
\rhead{\textsc{Group 003} -- rmc298, sjm324 -- \textsc{Page} \thepage}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{0.5in}

\title{CS 5220: Project 3 Initial Report}
\author{Group 003: Robert Chiodi (rmc298), Stephen McDowell (sjm324)}

\begin{document}
\thispagestyle{empty}
\maketitle

\section{Introduction}
For the third assignment, we employ techniques we have learned in previous assignments as well as explore the capabilities of the Message Passing Interface (MPI) in the context of the \href{https://en.wikipedia.org/wiki/Floyd\%E2\%80\%93Warshall_algorithm}{\textcolor{blue}{\underline{Floyd-Warshall}}} algorithm for computing all pairwise shortest paths.  As described in the assignment description, this assignment is very similar to the matrix multiplication algorithm we have studied previously in terms of overall problem setup, and only differs in the actual computations being performed.\\

\noindent We begin by focusing our efforts on using MPI to allow computation across multiple nodes on the \emph{totient} cluster, and then discuss optimizations of the algorithm to speedup the computation.  We end with a discussion of offloading the computation to the Xeon Phi's, and a summary of our tactics.\\

\noindent It is worth mentioning at this point that the assignment asked us to implement a parallel MPI algorithm, and then choose between optimizing the given OpenMP implementation and our own MPI implementation.  We choose to do both for two reasons:

\begin{enumerate}[1.]
  \item The optimizations for the OpenMP implementation are very much the same as the MPI implementation in that the algorithm being executed is the same -- the only difference being how threads are delegated work, and
  \item Recent trends in high performance computing fall more along the lines of ``first, use MPI to coordinate between multiple nodes, then use OpenMP on each node to further parallelize the work.''
\end{enumerate}

\section{Optimizations}

\section{Message Passing Interface (MPI)}
The method of MPI parallelization was chosen for its ease of implementation and decent performance. To start, the Floyd-Warshall pathing algorithm, originally parallelized using OpenMP, was changed in order to make use of MPI. First, the parallelization and delegation of work was moved outside of the square function to the main function, in order to eliminate multiple parallel initialization costs. From this point, the pathing matrix is split into $\mathtt{npx}\times \mathtt{npy}$ subdomains, where \texttt{npx} and \texttt{npy} are the number of processors used to divide the matrix in the horizontal and vertical directions, respectively. \texttt{MPI\_CART} commands are used to organize these threads in an optimal orientation and provide coordinates for the responsibilities of each processor, which are then used to determine the starting and ending indices of each threads subdomain in relation to the global pathing matrix. At this point, each thread has its own copy of the entire pathing matrix and calls the \texttt{shortest\_paths} function. \\

\noindent The functions \texttt{infinitize} and \texttt{deinfinitize}, as well as the diagonal zeroing loop, were not parallelized due to their relatively little computational expense. In doing so, the associated parallel communication costs were avoided. The values of \texttt{l} are then copied into \texttt{lnew} using \texttt{MPI\_ALLREDUCE} with the \texttt{MPI\_MIN} operator. In this way, consistency between processors is ensured. It would also be easily possible to parallelize \texttt{infinitize} and \texttt{deinfinitize} with this method of communication, if the problem size became large enough to warrant the additional communication costs associated with it. \\

\noindent In the main loop, the \texttt{square} function is called by each thread, where each thread determines the shortest paths for its subdomain. Once each thread is finished inside square, \texttt{MPI\_ALLREDUCE} is once again used with the \texttt{MPI\_MIN} operator to determine if all processors returned a value of 1, indicating all shortest paths have been found. The array \texttt{lnew} is then communicated and stored in \texttt{l}, once again using \texttt{MPI\_ALLREDUCE} with the \texttt{MPI\_MIN} operator. \\

While this is not the most efficient MPI implementation, we believe it to be sufficient to show that shared memory OpenMP will be significantly faster than MPI for problem sizes that fit on one node. This is due to the nature of the pathing algorithm, where every point to the left, right, top, and bottom of the element of interest needs to be checked to determine if it provides a shorter path, making this more efficient with shared memory. If the problem sizes driven by an application scaled greatly so that they could no longer fit on one node, the MPI implementation could be rewritten to provide some marginal gains in performance and noticeable gains in memory efficiency, enabling the algorithm to be run on very large problem sizes. \\

\noindent Due to the ability of MPI to utilize several nodes, we believe a heterogeneous parallelization strategy would be ideal, where MPI is used for inter-node communications and OpenMP is otherwise used to compute on the Xeon Phi cores. At this time, however, we have been unable to run on multiple nodes of Totient using MPI, preventing us from implementing this.

\subsection{MPI Scaling Study}
In order to understand the performance of the MPI-parallelized shortest paths algorithm, strong and weak scaling studies were run. For the strong scaling, six simulations were run for a problem size of $4000 \times 4000$ for four separate probabilities, 0.025, 0.05, 0.1, 0.3. The results are plotted in Figure~\ref{mpi_ss}.

\begin{figure}
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			#set key at 15.8,41
			#set size 1.0,0.5              
			unset log                          
			unset label                          
			#set xtic auto offset 0,-0.5 font ",10"                     
			#set ytic 25 font ",10" 
			#Set Info
			set xlabel "Number of Threads"
			#set xlabel offset 0,-0.5
			set ylabel "Scaling Efficiency"
			#set ylabel offset -1.25,0
			#set xr [0.0:16.0]
			#set yr [-50.0:50.0]
			plot "./benchmarking/mpi/strong/p025strong.txt" u 1:(490.5691/$2/$1) t 'p = 0.025' w linespoints, \
			"./benchmarking/mpi/strong/p05strong.txt" u 1:(538.9763960838318/$2/$1) t 'p = 0.050' w lp, \
			"./benchmarking/mpi/strong/p10strong.txt" u 1:(325.8776059150/$2/$1) t 'p = 0.100' w lp, \
			"./benchmarking/mpi/strong/p30strong.txt" u 1:(325.54634/$2/$1) t 'p = 0.300' w lp
		\end{gnuplot}
		\caption{Strong scaling for MPI parallelized path algorithm with $\mathtt{n} = 4000$.}
		\label{mpi_ss}
	\end{center}
\end{figure}


\section{Offloading to the Phi's}

\section{Summary}


\end{document}
