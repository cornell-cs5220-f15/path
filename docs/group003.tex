\documentclass[11pt]{article}
\usepackage{geometry}
\geometry{letterpaper}
\geometry{margin=0.5in}


\usepackage[utf8]{inputenc}
\usepackage{listings}
% \usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
% credit: http://tex.stackexchange.com/questions/68091/how-do-i-add-syntax-coloring-to-my-c-source-code-in-beamer
% https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
\lstset{language=C,
        basicstyle=\ttfamily,
        keywordstyle=\color{blue}\ttfamily,
        stringstyle=\color{red}\ttfamily,
        commentstyle=\color{OliveGreen}\ttfamily,
        morecomment=[l][\color{black}]{\#}
}

\title{\textbf{CS 5220 Project 1 Final Report}}
\author{Stephen McDowell (sjm324)}
\date{\textbf{September 2015}}

\usepackage{natbib}
\usepackage[miktex]{gnuplottex}
\usepackage{graphicx}
\usepackage{color}
\usepackage{pdfpages}

\usepackage{enumerate}

\newcommand{\tab}{\hspace*{2em}}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\sub}{\textsubscript}
\newcommand{\Depth}{2}
\newcommand{\Height}{2}
\newcommand{\Width}{2}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{relsize}
\usepackage{pdfpages}
\usepackage{hyperref}

\def\wl{\par \vspace{\baselineskip}}

\makeatletter
\renewcommand{\maketitle}{\bgroup\setlength{\parindent}{0pt}
\begin{flushleft}
  {\Large \textsc{\@title}}\newline
  \textsc{\@author}
  \rule{\textwidth}{1pt}
\end{flushleft}\egroup
}
\makeatother

\usepackage{fancyhdr}
\pagestyle{fancy}
\rhead{\textsc{Group 003} -- rmc298, sjm324 -- \textsc{Page} \thepage}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{0.5in}

\title{CS 5220: Project 3 Initial Report}
\author{Group 003: Robert Chiodi (rmc298), Stephen McDowell (sjm324)}

\begin{document}
\thispagestyle{empty}
\maketitle

\section{Introduction}
For the third assignment, we employ techniques we have learned in previous assignments as well as explore the capabilities of the Message Passing Interface (MPI) in the context of the \href{https://en.wikipedia.org/wiki/Floyd\%E2\%80\%93Warshall_algorithm}{\textcolor{blue}{\underline{Floyd-Warshall}}} algorithm for computing all pairwise shortest paths.  As described in the assignment description, this assignment is very similar to the matrix multiplication algorithm we have studied previously in terms of overall problem setup, and only differs in the actual computations being performed.\\

\noindent We begin by focusing our efforts on using MPI to allow computation across multiple nodes on the \emph{totient} cluster, and then discuss optimizations of the algorithm to speedup the computation.  We end with a discussion of offloading the computation to the Xeon Phi's, and a summary of our tactics.\\

\noindent It is worth mentioning at this point that the assignment asked us to implement a parallel MPI algorithm, and then choose between optimizing the given OpenMP implementation and our own MPI implementation.  We choose to do both for two reasons:

\begin{enumerate}[1.]
  \item The optimizations for the OpenMP implementation are very much the same as the MPI implementation in that the algorithm being executed is the same -- the only difference being how threads are delegated work, and
  \item Recent trends in high performance computing fall more along the lines of ``first, use MPI to coordinate between multiple nodes, then use OpenMP on each node to further parallelize the work.''
\end{enumerate}

\section{Optimizations}

There are two general forms of optimizations that we employ:

\begin{enumerate}[1.]
  \item Vectorization and/or compilation optimizations, and
  \item Algorithmic optimizations and/or enhancements.
\end{enumerate}

\noindent Each provide their own flavor of performance increase, but (1) is not as applicable for this assignment as it has been in the past since we don't have a large concern of optimizing the throughput of floating point operations.  As such, after a few modifications, the majority of optimization efforts should be focused on (2).

\subsection{Vectorization and Compilation Strategies}

Though at a first glance there do not seem to be many opportunities for vectorization, we elect to give \texttt{icc} the hints it needs to optimize as much as possible.  To enable portability, the following compilation hints are defined:

\begin{lstlisting}
#ifdef __INTEL_COMPILER
    #define DEF_ALIGN(x) __declspec(align((x)))
    #define USE_ALIGN(var, align) __assume_aligned((var), (align));
#else // GCC
    #define DEF_ALIGN(x) __attribute__ ((aligned((x))))
    /* __builtin_assume_align is unreliabale... */
    #define USE_ALIGN(var, align) ((void)0)
#endif
\end{lstlisting}

\noindent These two must be used in lock-step.  When declaring a variable of a specific alignment, we use \texttt{DEF\_ALIGN} to declare that this variable has a specific alignment.  Example usage would be something like

\begin{lstlisting}
  DEF_ALIGN(BYTE_ALIGN)
  int * restrict lnew = (int *)_mm_malloc(num_bytes, BYTE_ALIGN);
\end{lstlisting}

\noindent noting that declaring alignment is not enough, it must be enforced through either aligned allocators or \texttt{\_mm\_malloc}.  However, declaring alignment alone is not enough to enable the compiler top make the proper decisions when arranging loops / vectorizing / all of the other magic it does!  For example, when you pass a variable to a function, you need to indicate the the compiler that it has a given alignment.  For example,

\begin{lstlisting}
int square(int n,                 // Number of nodes
           int * restrict l,      // Partial distance at step s
           int * restrict lnew) { // Partial distance at step s+1

    USE_ALIGN(l,    BYTE_ALIGN);
    USE_ALIGN(lnew, BYTE_ALIGN);
    .
    .
    .
}
\end{lstlisting}

\noindent When used correctly, the \texttt{DEF} -- \texttt{USE} strategy can be quite effective at optimizing almost anything.  Comparing to the base code, including this simple two-phase process gives the following increases:

\begin{center}
  THIS IS A FIGURE
\end{center}

\subsection{Algorithmic Optimizations}

\noindent The most immediate and obvious form of algorithmic optimization for this assignment is to employ the \texttt{matmul-} style sub-blocking we are all familiar with.  This section will be detailed at a later time after we have incorporated this in our program.  Since this technique is familiar at this point, we elected to focus our efforts on MPI and elsewhere for the initial report.\\

\noindent At this time we feel it only worth mentioning that arbitrarily sized sub-blocks are a desireable feature, and that when considering the dimensions of the sub-blocks to employ a simple enhancement can be to have rectangular blocks that are larger in the $x$ direction than in the $y$.  This enables unit-stride access for a greater number of elements when doing the actual computation.

\section{Message Passing Interface (MPI)}
The method of MPI parallelization was chosen for its ease of implementation and decent performance. To start, the Floyd-Warshall pathing algorithm, originally parallelized using OpenMP, was changed in order to make use of MPI. First, the parallelization and delegation of work was moved outside of the square function to the main function, in order to eliminate multiple parallel initialization costs. From this point, the pathing matrix is split into $\mathtt{npx}\times \mathtt{npy}$ subdomains, where \texttt{npx} and \texttt{npy} are the number of processors used to divide the matrix in the horizontal and vertical directions, respectively. \texttt{MPI\_CART} commands are used to organize these threads in an optimal orientation and provide coordinates for the responsibilities of each processor, which are then used to determine the starting and ending indices of each threads subdomain in relation to the global pathing matrix. At this point, each thread has its own copy of the entire pathing matrix and calls the \texttt{shortest\_paths} function. \\

\noindent The functions \texttt{infinitize} and \texttt{deinfinitize}, as well as the diagonal zeroing loop, were not parallelized due to their relatively little computational expense. In doing so, the associated parallel communication costs were avoided. The values of \texttt{l} are then copied into \texttt{lnew} using \texttt{MPI\_ALLREDUCE} with the \texttt{MPI\_MIN} operator. In this way, consistency between processors is ensured. It would also be easily possible to parallelize \texttt{infinitize} and \texttt{deinfinitize} with this method of communication, if the problem size became large enough to warrant the additional communication costs associated with it. \\

\noindent In the main loop, the \texttt{square} function is called by each thread, where each thread determines the shortest paths for its subdomain. Once each thread is finished inside square, \texttt{MPI\_ALLREDUCE} is once again used with the \texttt{MPI\_MIN} operator to determine if all processors returned a value of 1, indicating all shortest paths have been found. The array \texttt{lnew} is then communicated and stored in \texttt{l}, once again using \texttt{MPI\_ALLREDUCE} with the \texttt{MPI\_MIN} operator. \\

\noindent While this is not the most efficient MPI implementation, we believe it to be sufficient to show that shared memory OpenMP will be significantly faster than MPI for problem sizes that fit on one node. This is due to the nature of the pathing algorithm, where every point to the left, right, top, and bottom of the element of interest needs to be checked to determine if it provides a shorter path, making this more efficient with shared memory. If the problem sizes driven by an application scaled greatly so that they could no longer fit on one node, the MPI implementation could be rewritten to provide some marginal gains in performance and noticeable gains in memory efficiency, enabling the algorithm to be run on very large problem sizes. \\

\noindent Due to the ability of MPI to utilize several nodes, we believe a heterogeneous parallelization strategy would be ideal, where MPI is used for inter-node communications and OpenMP is otherwise used to compute on the Xeon Phi cores. At this time, however, we have been unable to run on multiple nodes of Totient using MPI, preventing us from implementing this.

\subsection{MPI Scaling Study}
In order to understand the performance of the MPI-parallelized shortest paths algorithm, strong and weak scaling studies were run. For the strong scaling, six simulations were run for a problem size of $4000 \times 4000$ for four separate probabilities, 0.025, 0.05, 0.1, 0.3. The results are plotted in Figure~\ref{mpi_ss}.

\begin{figure}
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			#set key at 15.8,41
			#set size 1.0,0.5              
			unset log                          
			unset label                          
			#set xtic auto offset 0,-0.5 font ",10"                     
			#set ytic 25 font ",10" 
			#Set Info
			set xlabel "Number of Threads"
			#set xlabel offset 0,-0.5
			set ylabel "Scaling Efficiency"
			#set ylabel offset -1.25,0
			#set xr [0.0:16.0]
			#set yr [-50.0:50.0]
			plot "./benchmarking/mpi/strong/p025strong.txt" u 1:(490.5691/$2/$1) t 'p = 0.025' w linespoints, \
			"./benchmarking/mpi/strong/p05strong.txt" u 1:(538.9763960838318/$2/$1) t 'p = 0.050' w lp, \
			"./benchmarking/mpi/strong/p10strong.txt" u 1:(325.8776059150/$2/$1) t 'p = 0.100' w lp, \
			"./benchmarking/mpi/strong/p30strong.txt" u 1:(325.54634/$2/$1) t 'p = 0.300' w lp
		\end{gnuplot}
		\caption{Strong scaling for MPI parallelized path algorithm with $\mathtt{n} = 4000$.}
		\label{mpi_ss}
	\end{center}
\end{figure}


\section{Offloading to the Phi's}

The following compilation hint has been defined:

\begin{lstlisting}
#ifdef __MIC__
    #define BYTE_ALIGN 64
#else
    #define BYTE_ALIGN 32
#endif
\end{lstlisting}

\noindent This conditional definition is to account for the different preferred byte alignment of the Node (Xeon) and the Device (Phi).  We need to more carefully inspect the legitimacy of this tactic, because what this will do currently is the following:

\begin{enumerate}[1.]
  \item When the code is compiled for the Node, the byte alignment will be 32.
  \item When the code is compiled for the Device, the byte alignment will be 64.
\end{enumerate}

\noindent This is because, where \texttt{\#pragma offload target(mic)} sections are concerned, the code is compiled in two passes by \texttt{icc} -- once for the Node and once for the Device.  Although the above specified alignments are the respective preferred alignments, we are breaking the rule of enforcement here and will have to revisit how to appropriately do this.  Since we aim to send the majority of the computation on the Phi's, we may elect to just define the byte alignment to be 64 to guarantee that the Phi gets its preferences.

\section{Summary}


\end{document}
