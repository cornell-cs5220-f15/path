\documentclass[11pt]{article}
\usepackage[letterpaper,left=1in,right=1in,top=1in,bottom=1in]{geometry}

\usepackage[utf8]{inputenc}
\usepackage{listings}
% \usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
% credit: http://tex.stackexchange.com/questions/68091/how-do-i-add-syntax-coloring-to-my-c-source-code-in-beamer
% https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
\lstset{language=C,
        basicstyle=\ttfamily,
        keywordstyle=\color{blue}\ttfamily,
        stringstyle=\color{red}\ttfamily,
        commentstyle=\color{OliveGreen}\ttfamily,
        morecomment=[l][\color{black}]{\#},
        tabsize=2
}

\title{\textbf{CS 5220 Project 1 Final Report}}
\author{Stephen McDowell (sjm324)}
\date{\textbf{September 2015}}

\usepackage{natbib}
\usepackage[miktex]{gnuplottex}
\usepackage{graphicx}
\usepackage{color}
\usepackage{pdfpages}
\usepackage{epstopdf}

\usepackage{enumerate}

\newcommand{\tab}{\hspace*{2em}}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\sub}{\textsubscript}
\newcommand{\Depth}{2}
\newcommand{\Height}{2}
\newcommand{\Width}{2}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{relsize}
\usepackage{hyperref}

\def\wl{\par \vspace{\baselineskip}}

\makeatletter
\renewcommand{\maketitle}{\bgroup\setlength{\parindent}{0pt}
\begin{flushleft}
  {\Large \textsc{\@title}}\newline
  \textsc{\@author}
  \rule{\textwidth}{1pt}
\end{flushleft}\egroup
}
\makeatother

\usepackage{fancyhdr}
\pagestyle{fancy}
\rhead{\textsc{Group 003} -- rmc298, sjm324 -- \textsc{Page} \thepage}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{0.5in}

\title{CS 5220: Project 3 Initial Report}
\author{Group 003: Robert Chiodi (rmc298), Stephen McDowell (sjm324)}

\begin{document}
\thispagestyle{empty}
\maketitle

\section{Introduction}
For the third assignment, we employ techniques we have learned in previous assignments as well as explore the capabilities of the Message Passing Interface (MPI) in the context of the \href{https://en.wikipedia.org/wiki/Floyd\%E2\%80\%93Warshall_algorithm}{\textcolor{blue}{\underline{Floyd-Warshall}}} algorithm for computing all pairwise shortest paths.  As described in the assignment description, this assignment is very similar to the matrix multiplication algorithm we have studied previously in terms of overall problem setup, and only differs in the actual computations being performed.\\

\noindent We begin by focusing our efforts on using MPI to allow computation across multiple nodes on the \emph{totient} cluster, and then discuss optimizations of the algorithm to speedup the computation.  We end with a discussion of offloading the computation to the Xeon Phi's, and a summary of our tactics.\\

\noindent It is worth mentioning at this point that the assignment asked us to implement a parallel MPI algorithm, and then choose between optimizing the given OpenMP implementation and our own MPI implementation.  We choose to do both for two reasons:

\begin{enumerate}[1.]
  \item The optimizations for the OpenMP implementation are very much the same as the MPI implementation in that the algorithm being executed is the same -- the only difference being how threads are delegated work, and
  \item Recent trends in high performance computing fall more along the lines of ``first, use MPI to coordinate between multiple nodes, then use OpenMP on each node to further parallelize the work.''
\end{enumerate}

\section{Optimizations}
There are two general forms of optimizations that we employ:

\begin{enumerate}[1.]
  \item Vectorization and/or compilation optimizations, and
  \item Algorithmic optimizations and/or enhancements.
\end{enumerate}

\noindent Each provide their own flavor of performance increase, but (1) is not as applicable for this assignment as it has been in the past since we do not have a large concern of optimizing the throughput of floating point operations.  As such, after a few modifications, the majority of optimization efforts should be focused on (2).

\subsection{Vectorization and Compilation Strategies}

Though at a first glance there do not seem to be many opportunities for vectorization, we elect to give \texttt{icc} the hints it needs to optimize as much as possible.  To enable portability, the following compilation hints are defined:

\begin{lstlisting}
#ifdef __INTEL_COMPILER
    #define DEF_ALIGN(x) __declspec(align((x)))
    #define USE_ALIGN(var, align) __assume_aligned((var), (align));
#else // GCC
    #define DEF_ALIGN(x) __attribute__ ((aligned((x))))
    /* __builtin_assume_align is unreliabale... */
    #define USE_ALIGN(var, align) ((void)0)
#endif
\end{lstlisting}

\noindent These two must be used in lock-step.  When declaring a variable of a specific alignment, we use \texttt{DEF\_ALIGN} to declare that this variable has a specific alignment.  An example usage would be 
\begin{lstlisting}
  DEF_ALIGN(BYTE_ALIGN)
  int * restrict lnew = (int *)_mm_malloc(num_bytes, BYTE_ALIGN);
\end{lstlisting}

\noindent noting that declaring alignment is not enough, it must be enforced through either aligned allocators or \texttt{\_mm\_malloc}.  However, declaring alignment alone is not enough to enable the compiler to make the proper decisions when arranging loops and vectorizing. For example, when you pass a variable to a function, you need to indicate to the compiler that it has a given alignment, as in

\begin{lstlisting}
int square(int n,                 // Number of nodes
           int * restrict l,      // Partial distance at step s
           int * restrict lnew) { // Partial distance at step s+1

    USE_ALIGN(l,    BYTE_ALIGN);
    USE_ALIGN(lnew, BYTE_ALIGN);
    .
    .
    .
}
\end{lstlisting}

\noindent When used correctly, the \texttt{DEF} -- \texttt{USE} strategy can be quite effective at optimizing, however for this program it does not appear to be too effective.  Comparing to the base code, including this simple two-phase process gives the following increases:
\begin{table}[h]
	\begin{center}
		\begin{tabular}{|c| c| c| c|}
			\hline
			\# of Threads & Basic Time (s) & Optimized Time (s) & \% Increase \\ \hline
			1 &  538.585 & 538.384 & 0.04\%	\\ \hline
			2 &  256.654 &  261.027& -1.70\%	\\ \hline
			4 & 123.608  & 119.715 & 3.15\%	\\ \hline
			8 & 62.932  & 62.926 & 	0.01\%	\\ \hline
			16 & 47.883  & 44.550& 	6.96\%	\\ \hline
			24 & 41.951 & 44.717 & 	-6.593\%	\\ \hline
						
		\end{tabular}
	\end{center}
	\caption{Comparison between base code and optimized code speeds for $n = 4000$ and probability of $p = 0.05$.}
\end{table}


\noindent The only other optimization at this level that we tried was with respect to the branching in the square function:

\begin{lstlisting}
    if (lik + lkj < lij) {
        lij = lik+lkj;
        done = 0;
    }
\end{lstlisting}

\noindent Basic parallel programming techniques dictate that branches are the death of all parallel algorithms -- they induce thread divergence and prevent effective chaining of the parallel algorithm.  Our remedy was to remove the branching using the following strategy:

\begin{lstlisting}
    // https://graphics.stanford.edu/
    //~seander/bithacks.html#IntegerMinOrMax
    // If you know that INT_MIN <= x - y <= INT_MAX,
    // then you can use the following, which are faster
    // because (x - y) only needs to be evaluated once.
    int sum  = lik + lkj;
    int prev = lij;
    // min(sum, lij)
    lij = lij + ((sum - lij) & ((sum - lij) >>
     (sizeof(int) * CHAR_BIT - 1)));
    done = done && sum >= prev;
\end{lstlisting}

\noindent Though this removes the branching in the code, it actually performs significantly worse.  The reason becomes clear after noticing two things.  First, the introduction of significantly more instructions in this loop (as examined by using \texttt{objdump}):

\begin{lstlisting}
216d: 04 7f                 add    $0x7f,%al
216f: 00 0d 0e 73 75 6d     add    %cl,0x6d75730e
2175: 2e 31 32              xor    %esi,%cs:(%rdx
2178: 34 32                 xor    $0x32,%al
217a: 5f                    pop    %rdi
217b: 56                    push   %rsi
217c: 24 31                 and    $0x31,%al
217e: 65 00 0a              add    %cl,%gs:(%rdx)
2181: 04 00                 add    $0x0,%al
2183: 04 00                 add    $0x0,%al
2185: 00 56 6e              add    %dl,0x6e(%rsi)
2188: 82                    (bad)
2189: 88 00                 mov    %al,(%rax)
218b: 00 06                 add    %al,(%rsi)
218d: 04 7f                 add    $0x7f,%al
218f: 00 0e                 add    %cl,(%rsi)
2191: 0f 70 72 65 76        pshufw $0x76,0x65(%rd
2196: 2e 31 32              xor    %esi,%cs:(%rdx
2199: 34 32                 xor    $0x32,%al
219b: 5f                    pop    %rdi
219c: 56                    push   %rsi
219d: 24 31                 and    $0x31,%al
219f: 66                    data16
21a0: 00 0b                 add    %cl,(%rbx)
21a2: 04 00                 add    $0x0,%al
21a4: 04 00                 add    $0x0,%al
21a6: 00 56 6e              add    %dl,0x6e(%rsi)
21a9: 82                    (bad)
21aa: 88 00                 mov    %al,(%rax)
21ac: 00 06                 add    %al,(%rsi)
21ae: 04 7f                 add    $0x7f,%al
21b0: 01 4d 5f              add    %ecx,0x5f(%rbp
21b3: 00 01                 add    %al,(%rcx)
21b5: 00 00                 add    %al,(%rax)
21b7: 00 02                 add    %al,(%rdx)
21b9: 4d 5f                 rex.WRB pop %r15
21bb: 01 02                 add    %eax,(%rdx)
\end{lstlisting}

\noindent The other element that must be observed here is that x86 and later have instructions for conditional move / set, for which the branch above actually reduces to a small set of simple instructions.  That is, although in code we write a branch using an if statement, it gets reduced to a consistent number of instructions, since the logic inside of the branch is simple enough for the compiler to reduce it to a branchless set of instructions anyway.

\subsection{OpenMP Scaling Study}
In order to analyze the efficiency of parallelizing this shortest paths algorithm with OpenMP, a strong and weak scaling study was performed. This will be used alongside a scaling study for our MPI implementation, presented later, to determine the best method for parallelization. \\

\noindent First, we performed a strong scaling study to see the potential speedup that can be gained by using more processors. For this, we define the strong scaling as
\begin{equation}
\mathrm{Strong \; Scaling} = \frac{t_{\mathrm{serial}}}{t_{\mathrm{parallel}}} \; ,
\label{strongscale}
\end{equation}
and a strong scaling efficiency as 
\begin{equation}
\mathrm{Strong \; Scaling \; Efficiency} = \frac{t_{\mathrm{serial}}}{t_{\mathrm{parallel}}} \frac{1}{p} 
\label{normss}
\end{equation}
where $p$ is the number if processors. To enable easy comparison, we will use the strong scaling efficiency, which is normalized by a linear (ideal) speedup. The speedup of OpenMP derived from our study can be seen in Figure~\ref{omp_ss}.
\begin{figure}[h!]
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			#set key at 15.8,41
			set size 1.0,0.75              
			unset log                          
			unset label                          
			#set xtic auto offset 0,-0.5 font ",10"                     
			#set ytic 25 font ",10" 
			#Set Info
			set xlabel "Number of Threads"
			#set xlabel offset 0,-0.5
			set ylabel "Scaling Efficiency"
			#set ylabel offset -1.25,0
			#set xr [0.0:16.0]
			set yr [0.0:1.5]
			plot "./benchmarking/vectorized/strong/p025strong.txt" u 1:(489.7618279/$2/$1) t 'p = 0.025' w linespoints, \
			"./benchmarking/vectorized/strong/p05strong.txt" u 1:(538.3848/$2/$1) t 'p = 0.050' w lp, \
			"./benchmarking/vectorized/strong/p10strong.txt" u 1:(359.1135909/$2/$1) t 'p = 0.100' w lp, \
			"./benchmarking/vectorized/strong/p30strong.txt" u 1:(326.293136119/$2/$1) t 'p = 0.300' w lp
		\end{gnuplot}
		\caption{Strong scaling for optimized OpenMP parallelized path algorithm with $\mathtt{n} = 4000$.}
		\label{omp_ss}
	\end{center}
\end{figure}
\noindent While it is strange that the strong scaling efficiency rises over one for several points, this is possibly due to heterogeneity in the cluster. In order to quantify the deviation in time with constant parameters, the program should be run several times, however we have not had the time available to conduct this study. The OpenMP parallelization does seem to show constant scaling efficiency decrease, with no discontinuity between 8 and 16 processors, where threads from two different Xeon chips begin to be used.

\noindent A weak scaling was also performed in order to see the significance of communication for the OpenMP parallelized program. The parameters for this study can be seen in Figure~\ref{mpi_ws_tab}, with each run of the program being done four times for probabilities of 0.025, 0.05, 0.10, and 0.30. The results of this study can be seen in Figure~\ref{omp_ws} where weak scaling is calculated as
\begin{equation}
\mathrm{Weak \; Scaling} = \frac{t_{\mathrm{serial}}(n(p))}{t_{\mathrm{parallel}}(n(p),p)}
\label{ws}
\end{equation}
\begin{table}[h]
	\begin{center}
		\begin{tabular}{|c| c|}
			\hline
			\# of Threads & n \\ \hline
			1 & 1000 \\ \hline
			2 & 1414 \\ \hline
			4 & 2000 \\ \hline
			8 & 2828 \\ \hline
			16 & 4000 \\ \hline
			24 & 4899 \\ \hline
		\end{tabular}
	\end{center}
	\caption{Table of weak scaling parameters for number of threads and the squart root of the problem size ($n$)}
	\label{mpi_ws_tab}
\end{table}

\begin{figure}[h!]
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			#set key at 15.8,41
			set size 1.0,0.75              
			unset log                          
			unset label                          
			#set xtic auto offset 0,-0.5 font ",10"                     
			#set ytic 25 font ",10" 
			#Set Info
			set xlabel "Number of Threads"
			#set xlabel offset 0,-0.5
			set ylabel "Weak Scaling"
			#set ylabel offset -1.25,0
			#set xr [0.0:16.0]
			set yr [0.0:1.4]
			plot "./benchmarking/vectorized/weak/p025weak.txt" u 1:(2.9435491/$2) t 'p = 0.025' w linespoints, \
			"./benchmarking/vectorized/weak/p05weak.txt" u 1:(2.44597/$2) t 'p = 0.050' w lp, \
			"./benchmarking/vectorized/weak/p10weak.txt" u 1:(3.01424789428/$2) t 'p = 0.100' w lp, \
			"./benchmarking/vectorized/weak/p30weak.txt" u 1:(1.6850919/$2) t 'p = 0.300' w lp
		\end{gnuplot}
		\caption{Weak scaling for optimized OpenMP parallelized path algorithm with program parameters given in Table~\ref{mpi_ws_tab}.}
		\label{omp_ws}
	\end{center}
\end{figure}
From Figure~\ref{omp_ws}, it appears that communication and parallel overhead is very costly in the OpenMP shortest paths algorithm. This is most likely due to the fact that the OpenMP team is launched each time inside the \texttt{square} function, which is called multiple times during each program execution. Once again, there is a point displaying super-linear weak scaling. This is due to the fact that there is 302 less nodes per processor, requiring less work per processor. Currently, we are unsure why this only occurs for one of the probabilities. 

\subsection{Algorithmic Optimizations}

\noindent The most immediate and obvious form of algorithmic optimization for this assignment is to employ the \texttt{matmul-} style sub-blocking we used in the first assignment.  This section will be detailed at a later time after we have incorporated this in our program.  Since this technique is familiar at this point, we elected to focus our efforts on MPI and elsewhere for the initial report.\\

\noindent At this time we feel it only worth mentioning that arbitrarily sized sub-blocks are a desirable feature, and that when considering the dimensions of the sub-blocks to employ a simple enhancement can be to have rectangular blocks that are larger in the horizontal than in the vertical.  This enables unit-stride access for a greater number of elements when doing the actual computation.

\section{Message Passing Interface (MPI)}

\noindent The method of MPI parallelization was chosen for its ease of implementation and decent performance. To start, the Floyd-Warshall pathing algorithm, originally parallelized using OpenMP, was changed in order to make use of MPI. First, the parallelization and delegation of work was moved outside of the square function and into the main function, in order to eliminate multiple parallel initialization costs. From this point, the pathing matrix is split into $\mathtt{npx}\times \mathtt{npy}$ subdomains, where \texttt{npx} and \texttt{npy} are the number of processors used to divide the matrix in the horizontal and vertical directions, respectively. \texttt{MPI\_CART} commands are used to organize these threads in an optimal orientation and provide coordinates for the responsibilities of each processor, which are then used to determine the starting and ending indices of each thread's subdomain in relation to the global pathing matrix. At this point, each thread has its own copy of the entire pathing matrix and calls the \texttt{shortest\_path} function. \\

\noindent The functions \texttt{infinitize} and \texttt{deinfinitize}, as well as the diagonal zeroing loop, were not parallelized due to their relatively little computational expense. In doing so, the associated parallel communication costs were avoided. The values of \texttt{l} are then copied into \texttt{lnew} using \texttt{MPI\_ALLREDUCE} with the \texttt{MPI\_MIN} operator. In this way, consistency between processors is ensured. It would also be possible to parallelize \texttt{infinitize} and \texttt{deinfinitize} with this method of communication, if the problem size became large enough to warrant the additional communication costs associated with it. \\

\noindent In the main loop of \texttt{shortest\_path}, the \texttt{square} function is called by each thread, where each thread determines the shortest paths for its subdomain. Once each thread is finished inside square, \texttt{MPI\_ALLREDUCE} is once again used with the \texttt{MPI\_MIN} operator to determine if all processors returned a value of 1, indicating all shortest paths have been found. The array \texttt{lnew} is then communicated and stored in \texttt{l}, once again using \texttt{MPI\_ALLREDUCE} with the \texttt{MPI\_MIN} operator. \\

\noindent While this is not the most efficient MPI implementation, we believe it to be sufficient to show that shared memory OpenMP will be significantly faster than MPI for problem sizes that fit on one node. This is due to the nature of the pathing algorithm, where every point to the left, right, top, and bottom of the element of interest needs to be checked to determine if it provides a shorter path, making this more efficient with shared memory. If the problem sizes driven by an application scaled greatly so that they could no longer fit on one node, the MPI implementation could be rewritten to provide some marginal gains in performance and noticeable gains in memory efficiency, enabling the algorithm to be run on very large problem sizes. \\

%\noindent Due to the ability of MPI to utilize several nodes, we believe a heterogeneous parallelization strategy would be ideal, where MPI is used for inter-node communications and OpenMP is otherwise used to compute on the Xeon Phi cores. At this time, however, we have been unable to run on multiple nodes of Totient using MPI, preventing us from implementing this.

\subsection{MPI Scaling Study}
In order to understand the performance of the MPI-parallelized shortest paths algorithm, strong and weak scaling studies were run. For the strong scaling, six simulations were run for a problem size of $4000 \times 4000$ for four separate probabilities: 0.025, 0.05, 0.1, 0.3. The results are plotted in Figure~\ref{mpi_ss}.

\begin{figure}[h]
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			#set key at 15.8,41
			set size 1.0,0.75              
			unset log                          
			unset label                          
			#set xtic auto offset 0,-0.5 font ",10"                     
			#set ytic 25 font ",10" 
			#Set Info
			set xlabel "Number of Threads"
			#set xlabel offset 0,-0.5
			set ylabel "Scaling Efficiency"
			#set ylabel offset -1.25,0
			#set xr [0.0:16.0]
			set yr [0.0:1.0]
			plot "./benchmarking/mpi/strong/p025strong.txt" u 1:(490.5691/$2/$1) t 'p = 0.025' w linespoints, \
			"./benchmarking/mpi/strong/p05strong.txt" u 1:(538.9763960838318/$2/$1) t 'p = 0.050' w lp, \
			"./benchmarking/mpi/strong/p10strong.txt" u 1:(325.8776059150/$2/$1) t 'p = 0.100' w lp, \
			"./benchmarking/mpi/strong/p30strong.txt" u 1:(325.54634/$2/$1) t 'p = 0.300' w lp
		\end{gnuplot}
		\caption{Strong scaling for MPI parallelized path algorithm with $\mathtt{n} = 4000$.}
		\label{mpi_ss}
	\end{center}
\end{figure}
\noindent A couple of comments can be made about the parallel efficiency shown in Figure~\ref{mpi_ss}. First, there is a stark decrease in scaling efficiency between 8 and 16 threads. This is most likely due to an increase in communication costs, as running the program with 8 threads only requires communication on the same Xeon chip, while 16 threads spans two Xeon chips. For all probabilities presented here, this actually leads to the program completing quicker when run with only 8 processors as opposed to 16. It is interesting to note that this discontinuity in scaling between 8 and 16 processors does not appear for OpenMP (see Figure~\ref{omp_ss}), possibly due to optimized communications inside the OpenMP library itself. A second comment that can be made is that scaling efficiency appears to be independent of the probability parameter. \\

\noindent Typically, weak scaling is used to view the communication costs associated with parallelizing a program since the algorithmic work should remain constant per processor during the study. This should allow us to see if communications become a dominant source of time in our program, requiring us to rethink our MPI parallelization strategy. The parameters for our weak scaling study can be seen in Table~\ref{mpi_ws_tab}, where each parameter configuration was one again run for probabilities of 0.025, 0.05, 0.1, and 0.3. The results of the study are shown in Figure~\ref{mpi_ws}

\begin{figure}[h]
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			#set key at 15.8,41
			set size 1.0,0.75              
			unset log                          
			unset label                          
			#set xtic auto offset 0,-0.5 font ",10"                     
			#set ytic 25 font ",10" 
			#Set Info
			set xlabel "Number of Threads"
			#set xlabel offset 0,-0.5
			set ylabel "Weak Scaling"
			#set ylabel offset -1.25,0
			#set xr [0.0:16.0]
			set yr [0.0:1.2]
			plot "./benchmarking/mpi/weak/p025weak.txt" u 1:(2.49040075/$2) t 'p = 0.025' w linespoints, \
			"./benchmarking/mpi/weak/p05weak.txt" u 1:(2.4700660705/$2) t 'p = 0.050' w lp, \
			"./benchmarking/mpi/weak/p10weak.txt" u 1:(2.49511981/$2) t 'p = 0.100' w lp, \
			"./benchmarking/mpi/weak/p30weak.txt" u 1:(1.6716897/$2) t 'p = 0.300' w lp
		\end{gnuplot}
		\caption{Weak scaling for MPI parallelized path algorithm with program parameters given in Table~\ref{mpi_ws_tab}.}
		\label{mpi_ws}
	\end{center}
\end{figure}
\noindent From the weak scaling, it can once again be seen that large communication costs are associated with using threads that span two Xeon chips by comparing the weak scaling at 8 threads and 16 threads. The poor performance indicates that if we desire to use the MPI parallelization as anything more than a message passer for OpenMP thread teams running on Xeon Phi's, significant effort should be spent on improving parallelization through reducing the communication frequency and the size of data communicated. The higher than one weak scaling for two threads with a probability of 0.1 is once again due to the fact that there is 302 less nodes in the system per processor. We are still unsure of the cause and do not yet have an explanation for why the super-linear scaling displayed in the MPI version is for a probability of 0.10, while for the OpenMP version it is super-linear for a probability of 0.025.  

\section{Post Initial Report Work}
\subsection{Blocking of the \texttt{square} function}
\noindent In order to reduce cache misses and increase memory locality, the \texttt{square} function in the pathing algorithm was broken up into blocks in the same manner as used for the general matrix multiply assignment. In order to increase unit striding in memory, blocks moving in the horizontal direction across the matrix (\texttt{i}) are set to be longer than the blocks for vertical movement (\texttt{j}) and the search for shorter paths (\texttt{k}). Additionally, in an effort to take advantage of unit strides in memory, the loop indices were reordered to have horizontal movement (\texttt{i}) be the innermost loop, meaning only unit stride in memory is performed in the inner loop. The new \texttt{square} function appears as
\begin{lstlisting}
    // Major Blocks
    for(int J = 0; J < n\_height; ++J) \{
	    for(int K = 0; K < n\_height; ++K) \{
		    for(int I = 0; I < n\_width; ++I)\{
			    // Calculate ending indices for the set of blocks
			    int j\_end   = ((J+1)*height\_size < n ? 
							    height\_size : (n-(J*height\_size)));
			    int k\_end   = ((K+1)*height\_size < n ?
							     height\_size : (n-(K*height\_size)));
			    int i\_end   = ((I+1)*width\_size  < n ?
							     width\_size  : (n-(I*width\_size)));
			    int j\_init  = J*height\_size*n;
			    int kn\_init = K*height\_size*n;
			    int k\_init  = K*height\_size;
			    int i\_init  = I*width\_size;
    
			    // Minor Blocks
			    for (int j = 0; j < j\_end; ++j) \{
				    int jn = j\_init+j*n;
    
				    for (int k = 0; k < k\_end; ++k) \{
					    int kn  = kn\_init+k*n;
					    int lkj = l[jn+k\_init+k];
    
					    for (int i = 0; i < i\_end; ++i) \{
						    int lij\_ind = jn+i\_init+i;
						    int lij = lnew[lij\_ind];
						    int lik = l[kn+i\_init+i];
    
						    if (lik + lkj < lij) \{
							    lij = lik+lkj;
							    lnew[lij\_ind] = lij;
							    done = 0;
						    \}
\end{lstlisting}
\noindent Rearranging the loop structure in this way reduces the working set, allowing it to fit in the L2 cache, greatly reducing cache misses. Where possible, indices have been precomputed outside of the innermost loop in order to reduce the amount of floating point operations used in index calculation, such as \texttt{jn} and \texttt{kn}. Testing from Group 19's ``matmul'' report (Robert Chiodi's old group) was directly used here in determining the ideal block sizes. One major change, however, was each block size was doubled since in this case, integers are used, which are half the size of the doubles used in assignment one. \\

\noindent While naive implementation of general matrix multiply blocking structure may not be ideal for this pathing algorithm, almost an order of magnitude acceleration is achieved over a wide range of parallelization for a moderate problem size of $n = 4000$. This can be seen in Figure~\ref{blocked_faster}, where the blocked code remains an order of magnitude faster than the original optimized code presented in our preliminary report. This indicates that memory access issues were a major bottleneck in the original code, which were alleviated through reordering loop indices and performing the computation in block to force memory locality.
\begin{figure}[h]
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			set logscale y
			set yr[1.0:100]
			plot "./benchmarking/blocked/strong/p025strong.txt" u 1:2 t 'Blocked p = 0.025' w lp, \
			"./benchmarking/blocked/strong/p05strong.txt" u 1:2 t 'Blocked p = 0.050' w lp, \
			"./benchmarking/vectorized/strong/p025strong.txt" u 1:2 t 'Initial p = 0.025' w lp, \
			"./benchmarking/vectorized/strong/p05strong.txt" u 1:2 t 'Initial p = 0.050' w lp
		\end{gnuplot}
		\caption{Comparison of runtimes for OpenMP with and without blocking. The problem size was $\mathtt{n} = 4000$.}
		\label{blocked_faster}
	\end{center}
\end{figure}
\newpage
\noindent While the blocking provided significant performance gains over the previous, unblocked pathing algorithm, it was not expected that this would aid in the parallel efficiency of the OpenMP method. This was proven to be true when strong and weak scaling studies were performed, shown below in Figure~\ref{blocked_ss} and~\ref{blocked_ws}, respectively. Once again, the strong scaling was performed using 4000 points on each side and probabilities of 0.025, 0.050, 0.10, and 0.30. The weak scaling was performed using the configurations given in Table~\ref{mpi_ws_tab}.

\begin{figure}[h]
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			#set key at 15.8,41
			set size 1.0,0.75              
			unset log                          
			unset label                          
			#set xtic auto offset 0,-0.5 font ",10"                     
			#set ytic 25 font ",10" 
			#Set Info
			set xlabel "Number of Threads"
			#set xlabel offset 0,-0.5
			set ylabel "Scaling Efficiency"
			#set ylabel offset -1.25,0
			#set xr [0.0:16.0]
			set yr [0.0:1.5]
			plot "./benchmarking/blocked/strong/p025strong.txt" u 1:(72.639657/$2/$1) t 'p = 0.025' w linespoints, \
			"./benchmarking/blocked/strong/p05strong.txt" u 1:(59.38055419/$2/$1) t 'p = 0.050' w lp, \
			"./benchmarking/blocked/strong/p10strong.txt" u 1:(39.7338488101/$2/$1) t 'p = 0.100' w lp, \
			"./benchmarking/blocked/strong/p30strong.txt" u 1:(48.6280469/$2/$1) t 'p = 0.300' w lp
		\end{gnuplot}
		\caption{Strong scaling for OpenMP parallelized path algorithm with blocked submatrices. The problem size was $\mathtt{n} = 4000$.}
		\label{blocked_ss}
	\end{center}
\end{figure}


\begin{figure}[h]
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			#set key at 15.8,41
			set size 1.0,0.75              
			unset log                          
			unset label                          
			#set xtic auto offset 0,-0.5 font ",10"                     
			#set ytic 25 font ",10" 
			#Set Info
			set xlabel "Number of Threads"
			#set xlabel offset 0,-0.5
			set ylabel "Weak Scaling"
			#set ylabel offset -1.25,0
			#set xr [0.0:16.0]
			set yr [0.0:1.2]
			plot "./benchmarking/blocked/weak/p025weak.txt" u 1:(1.01929498/$2) t 'p = 0.025' w linespoints, \
			"./benchmarking/blocked/weak/p05weak.txt" u 1:(1.18449306/$2) t 'p = 0.050' w lp, \
			"./benchmarking/blocked/weak/p10weak.txt" u 1:(1.25237/$2) t 'p = 0.100' w lp, \
			"./benchmarking/blocked/weak/p30weak.txt" u 1:(0.84293103/$2) t 'p = 0.300' w lp
		\end{gnuplot}
		\caption{Weak scaling for OpenMP parallelized path algorithm with blocked submatrices. Program parameters are given in Table~\ref{mpi_ws_tab}.}
		\label{blocked_ws}
	\end{center}
\end{figure}

\noindent By comparing these figures to the original OpenMP scaling figures (\ref{omp_ss} and \ref{omp_ws}), it is obvious that blocking had no effect on the parallel efficiency. Once again, we are not sure why strong scaling efficiency displays values greater than one, however it is possible that this is due to node inhomogeneity on totient, since each job was not submitted to a specific node. For the weak scaling case, this greater than one scaling is due to the limited communication costs when using two threads relative to one, coupled by the fact that the problem is not exactly scaled, meaning with two threads, each thread has 302 less nodes compared to the serial case, as discussed before. 

\subsection{Offloading to the Phi's}

\subsubsection{Compilation Assistance}

Since the Xeon Phi has a larger set of registers, it's preferred data alignment is 64 bytes.  As such, the files that offload work to the Phi (\texttt{path-blocked-device.c} and \texttt{path-blocked-naive.c}) define the following compilation hints:

\begin{lstlisting}
#define BYTE_ALIGN  64
#define width_size  512
#define height_size 128
\end{lstlisting}

\noindent Originally this was in a conditional \texttt{\#ifdef \_\_MIC\_\_} region, but after careful thought and analysis we conclude that since no real work was intended for the Nodes in these files, the conditional definitions had little meaning, for the following reasons

\begin{enumerate}[1.]
  \item When the code is compiled for the Node, the byte alignment will be 32.
  \item When the code is compiled for the Device, the byte alignment will be 64.
\end{enumerate}

\noindent This is because, where \texttt{\#pragma offload target(mic)} and \texttt{\_\_declspec(target(mic))} sections are concerned, the code is compiled in two passes by \texttt{icc} -- once for the Node and once for the Device.  Although the Node and Phi have different preferred alignments, the conditional definition violates what we assume to be true in the Phi code.  You cannot allocate data on the Node with a 32 byte alignment and assume it is 64 byte aligned after things get transferred to the Phi.\\

\noindent Similar to before, to enable portability to environments where \texttt{icc} is not present, we create the following definitions

\begin{lstlisting}
#ifdef __INTEL_COMPILER
    #define DEF_ALIGN(x) __declspec(align((x)))
    #define USE_ALIGN(var, align) __assume_aligned((var), (align));
    #define TARGET_MIC __declspec(target(mic))
#else // GCC
    #define DEF_ALIGN(x) __attribute__ ((aligned((x))))
    /* __builtin_assume_align is unreliabale... */
    #define USE_ALIGN(var, align) ((void)0)
    #define TARGET_MIC /* n/a */
#endif
\end{lstlisting}

\noindent Where the new addition is \texttt{TARGET\_MIC}.  This allows us to declare functions targeted for the Phi in the same way for both \texttt{icc} and \texttt{gcc}:

\begin{lstlisting}
TARGET_MIC
void solve(int n,                    // Number of nodes
           int * restrict orig_l,    // Partial distance at step s
           ...
\end{lstlisting}

\noindent The last bit of information the compiler needs is the \texttt{include} and \texttt{define} statements at the top, which can be surrounded with the alternative approach for declaring Phi regions:

\begin{lstlisting}
#pragma offload_attribute(push, target(mic))
#include <...>
...
#define ... ...
...
#pragma offload_attribute(pop)
\end{lstlisting}

\noindent Not all of the \texttt{include} statements necessarily need to be included, but this is simpler in that it enables more computation to be offloaded to the Phi in the future.

\subsubsection{Offloading -- A Naive First Approach}

\noindent With the same blocking code presented before, our first approach to offloading was to send the \texttt{square} function we identified as being the bottleneck of the algorithm straight to the Phi:

\begin{lstlisting}
#ifdef __INTEL_COMPILER
#pragma offload target(mic:0)                                 \
    in(n_threads)                                             \
    in(n)                                                     \
    in(n_width)                                               \
    in(n_height)                                              \
    inout(l    : length(n*n) alloc_if(first_iter) free_if(0)) \
    inout(lnew : length(n*n) alloc_if(first_iter) free_if(0))
#endif
    done = square(n, l, lnew, n_width, n_height, n_threads);
\end{lstlisting}

\noindent The \texttt{INTEL\_COMPILER} condition was needed for \texttt{gcc} compilation, likely because of the line continuations.  All this does is specify the parameters to \texttt{square}, noting that for both \texttt{l} and \texttt{lnew} we need those results copied back to the host in this naive approach.\\

\noindent The only optimization made here is recognizing that since this method is called potentially many times, we want to avoid allocating on the Phi each time.  On the first iteration we specify that the memory needs to be allocated, and for all other iterations we can keep the pointers that were allocated in the first run.  Correspondingly, though, it is important to free this memory outside of the \texttt{for(int done=0; !done;)} loop:

\begin{lstlisting}
#ifdef __INTEL_COMPILER
    // free the phi memory used in the loop
    #pragma offload_transfer target(mic:0)   \
                    nocopy(l : free_if(1))   \
                    nocopy(lnew : free_if(1))
#endif
\end{lstlisting}

\noindent At this point we observe a key flaw in this approach.  There is no need to continue copying over \texttt{l} and \texttt{lnew} every time that \texttt{square} is called.  One option that would be simple to implement is to change the length specifiers

\begin{lstlisting}
inout(l    : length(first_iter*n*n) ...
inout(lnew : length(first_iter*n*n) ...
\end{lstlisting}

\noindent This can be done because \texttt{first\_iter} will be \texttt{1} on the first iteration, and \texttt{0} for all other iterations.  The only extra piece of data that would be needed is a flag (say as a parameter to \texttt{square}) to indicate whethere this was an even or an odd iteration.  This will enable the usage of \texttt{l} and \texttt{lnew} on the Phi since they have already been allocated, but indicates to the dispatcher that no data needs to be copied.\\

\noindent We present now a strong and weak scaling study demonstrating the results of this approach:


\begin{figure}[h]
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			#set key at 15.8,41
			set size 1.0,0.75              
			unset log                          
			unset label                          
			#set xtic auto offset 0,-0.5 font ",10"                     
			#set ytic 25 font ",10" 
			#Set Info
			set xlabel "Number of Threads"
			#set xlabel offset 0,-0.5
			set ylabel "Scaling Efficiency"
			#set ylabel offset -1.25,0
			#set xr [0.0:16.0]
			set yr [0.0:1.5]
			plot "./benchmarking/blocked_phi/strong/p025strong.txt" u 1:(1350.99216/$2/$1*8) t 'p = 0.025' w linespoints, \
			"./benchmarking/blocked_phi/strong/p05strong.txt" u 1:(1790.88339304/$2/$1*4) t 'p = 0.050' w lp, \
			"./benchmarking/blocked_phi/strong/p10strong.txt" u 1:(1807.047016/$2/$1*4) t 'p = 0.100' w lp, \
			"./benchmarking/blocked_phi/strong/p30strong.txt" u 1:(1767.469844/$2/$1*4) t 'p = 0.300' w lp
		\end{gnuplot}
		\caption{Strong scaling for OpenMP parallelized path algorithm with blocked submatrices computed on the Xeon Phi coprocessors. The problem size was $\mathtt{n} = 11111$.}
		\label{blocked_device_ss}
	\end{center}
\end{figure}


\begin{figure}[h]
	\begin{center}
		\begin{gnuplot}[terminal=cairolatex, terminaloptions= color] 
			#set key at 15.8,41
			set size 1.0,0.75              
			unset log                          
			unset label                          
			#set xtic auto offset 0,-0.5 font ",10"                     
			#set ytic 25 font ",10" 
			#Set Info
			set xlabel "Number of Threads"
			#set xlabel offset 0,-0.5
			set ylabel "Weak Scaling"
			#set ylabel offset -1.25,0
			#set xr [0.0:16.0]
			set yr [0.0:1.2]
			plot "./benchmarking/blocked_phi/weak/p025weak.txt" u 1:(13.64914/$2) t 'p = 0.025' w linespoints, \
			"./benchmarking/blocked_phi/weak/p05weak.txt" u 1:(13.99326205/$2) t 'p = 0.050' w lp, \
			"./benchmarking/blocked_phi/weak/p10weak.txt" u 1:(9.705235004/$2) t 'p = 0.100' w lp, \
			"./benchmarking/blocked_phi/weak/p30weak.txt" u 1:(10.184929132/$2) t 'p = 0.300' w lp
		\end{gnuplot}
		\caption{\Huge Put parameters of study here for weak scaling or make a table and refer to it.}
		\label{blocked_device_ws}
	\end{center}
\end{figure}
\begin{center}
	{\Huge PLOT PLOT PLOT}
\end{center}

\subsubsection{Offloading to the Phi -- A New Perspective}

\noindent At this point, we decided that a more optimal implementation would be to avoid the Node driving the Phi computation altogether, and just use a single \texttt{offload target(mic)} statement that will not terminate until the results are in.  The major difference being to basically take the \texttt{for(int done=0; !done;)} loop and put it in \texttt{square}.  At this point the purpose of the method has changed enough that it warrants a renaming, with additional parameters:

\begin{lstlisting}
TARGET_MIC
void solve(int n,                    // Number of nodes
           int * restrict orig_l,    // Partial distance at step s
           int * restrict orig_lnew, // Partial distance at step s+1
           int n_width,              // Width (x direction) of block
           int n_height,             // Height (y direction) of block
           int n_threads,            // how many threads to use
           int *copy_back) {         // copy back? 	
\end{lstlisting}

\noindent Where the new parameter of interest here is \texttt{int *copy\_back}.  This is just a single int that serves as a flag to the Node of whether the results of the computation ended on an even or an odd iteration, and therefore whether or not \texttt{lnew} needs to be copied back into \texttt{l} after the call to \texttt{solve} terminates inside of \texttt{shortest\_paths}.\\

\noindent To properly explain \texttt{copy\_back}, we will first explain the \texttt{solve} algorithm.  The method essentially amounts to the following changes:

\begin{lstlisting}
// since this is double buffering, we may have written the final results 
// into the alternate orig_lnew, and therefore need to copy them
// back at the end
*copy_back = 0;

// buffered variables
int *l, *lnew;
size_t step = 0;

// while(!done)
for(/* step = 0 */; /* true */ ; ++step) {
    // setup double buffers
    int even = step % 2;
    if(even) {
        l    = orig_l;
        lnew = orig_lnew;
    }
    else {
        l    = orig_lnew;
        lnew = orig_l;
    }

    // Major Blocks
    int done = 1;
    #pragma omp parallel for
    ... same blocking code ...
    }// end Major Blocks (and omp parallel for)
    if(done) break;
}

*copy_back = step % 2 == 0;
\end{lstlisting}

\noindent In theory, it would be much simpler to just have \texttt{copy\_back} be the return value of the \texttt{solve} function, but for whatever reason this would not compile.  Passing it as a pointer was a cheap hack to work around this issue.\\

\noindent So all of this turns into the statement that if \texttt{solve} had an even number of iterations, this means that the results were written into \texttt{lnew} -- and need to be copied back, and if there were an odd number of iterations then the results were written into \texttt{l} -- and do not need to be copied back.\\

\noindent Then in the \texttt{shortest\_paths} method, we can remove the \texttt{for(int done=0; !done;)} loop and instead just call one time

\begin{lstlisting}
// the phi code is double buffered; may need to copy back after
int copy_back = -1;
int *cb = &copy_back;

#ifdef __INTEL_COMPILER
#pragma offload target(mic:0)                            \
        in(n_threads)                                    \
        in(n)                                            \
        in(n_width)                                      \
        in(n_height)                                     \
        inout(cb   : length(1)   alloc_if(1) free_if(1)) \
        inout(l    : length(n*n) alloc_if(1) free_if(1)) \
        inout(lnew : length(n*n) alloc_if(1) free_if(1))
#endif
solve(n, l, lnew, n_width, n_height, n_threads, cb);

if(copy_back)
    memcpy(l, lnew, n*n * sizeof(int));
\end{lstlisting}

\end{document}
