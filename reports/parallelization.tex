\section{Parallelization}\label{sec:parallelization}
We parallelized the \rs{}, \block{}, and \fw{} algorithm using three
parallelization mechanisms: OpenMP, MPI, and a hybrid of the two. In this
section, we describe how each mechanism was used for parallelization in detail.

\subsection{OpenMP}
Parallelizing each algorithm using OpenMP is simply a matter of annotating the
outer loop of respective \texttt{square} functions with
\texttt{\#pragma omp parallel for}. The implementations of \rs{}, \block{}, and
\fw{} using OpenMP are in \texttt{rs-omp.c}, \texttt{block-omp.c}, and
\texttt{fw-omp.c}.

\subsection{MPI}
Parallelizing the algorithms using MPI requires decomposing the problem into sub
parts, which is similar to the idea of domain decomposition from the last project. We
drew inspiration from previous work about distance-vector routing using
the Bellman$-$Ford algorithm in determining shortest distances between routers. In that
algorithm, each router sends its distances to all other routers (each time
there is a change) to its neighbors which then use that information to update
their own distances. This continues until no routers have changed distances.

In our case, we probably do not have a single thread per node, as there may
not be enough hardware threads available. Instead, each ``router'' is an MPI
rank and is responsible for a set of nodes rather than a single node. Each MPI
rank calculates the minimum distance to each of its nodes from all other nodes
going through each node between 1 and $N$.

For \rs{} and \block{}, each rank also determines if any distances in its range have
changed. All MPI ranks then synchronize to gather distances from others
and determine if any distances have changed. If none have changed, then the
algorithm terminates and the ``master'' rank (rank 0) outputs checksum and
timing information. To synchronize the distances across all ranks, 
we use \texttt{mpi\_allgather} which sends each rank's distances to all other ranks 
and collects them from every rank including itself into a single buffer.
To determine if any distances have changed, we use
\texttt{mpi\_allreduce} on each rank's ``done'' variable.  The implementation of \fw{} must synchronize for
every iteration of $k$, and does not need to check if its finished as the
algorithm will always finish in the same number of loops ($N$).

The implementations of \rs{}, \block{}, and \fw{} using MPI are in
\texttt{rs-mpi.c}, \texttt{block-mpi.c}, and \texttt{fw-mpi.c}.

\subsection{Hybrid}
MPI and OpenMP interact seamlessly when put together, making it easy to combine
our MPI implementation with the release OpenMP implementation. Given a fixed
number of MPI ranks $r$, and $p$ available threads, then each MPI rank will
have access to $\frac{p}{r}$ threads which can be used in OpenMP parallel
sections of code. This means we do not take full advantage of all threads
available to us if $p$ is not divisible by $r$. We implemented a hybrid version
of each algorithm (by combining our MPI implementation with the OpenMP
portitons of the original implementation) in \texttt{rs-hybrid.c},
\texttt{block-hybrid.c}, and \texttt {fw-hybrid.c}.
