
\documentclass[11pt]{article}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{float}
\usepackage{inconsolata}
\usepackage{url}
\usepackage{titlesec}
\setcounter{secnumdepth}{4}
\setlength\parindent{0pt}
\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.5cm
\textwidth16.5cm
\footskip1.0cm

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}


\begin{document}
\title{CS 5220\\ Project 3 - All-Pairs Shortest Path}
\author{Eric Gao (emg222)\\ Elliot Cartee (evc34)\\ Sheroze Sheriffdeen(mss385)}
\maketitle

\section{Introduction}
The Floyd-Warshall algorithm computes the pair-wise shortest path lengths given a graph with a metric. The computational pattern of this algorithm is very much akin to matrix multiplication. If $l_{ij}^s$ represents the the length of the shortest path from node $i$ to $j$ in at most $2^s$ steps, then
\begin{equation}
	l_{ij}^{s+1} = \min_k \{ l_{ik}^s + l_{kj}^s \}
\end{equation} 
 represents the shortest path from $i$ to $j$ of at most $2^{j+1}$ hops. \cite{writeup} 
\section{Design Decisions}
 
\subsection{Parallel Tuning}

Since the Floyd-Warshall algorithm is structured very similarly to matrix multiplication, we decided to try taking some of the tuning methods used in the first project on matrix multiplication, and applying them to the Floyd-Warshall algorithm. \\

The first of these methods uses a blocking scheme, so that updating the shortest path lengths is done through repeated calls to a small kernel. Since the size of this small kernel is known at compile-time, the compiler is able to optimize this small kernel extremely efficiently.  \\

The second method changes the loop ordering, so that in the innermost loop, memory is being accessed with unit stride, which increases performance. \\

\subsection{Message Passing Interface}\label{sec:mpi}

In addition to the tuned parallel implementations, we explored an approach that uses the Message Passing Interface to achieve parallelism. In the MPI implementation, each process handles a certain region of the graph. To prevent a master process orchestrating the distance computation, we ideally want each process to only wait for information from the relevant part of the graph. To that end, we take the adjacency matrix on which the Floyd-Warshall algorithm is run and partition the graph by chunks of columns. Then each processors is responsible for update a contiguous sequence of columns in the matrix.

\begin{figure}[H]
\centering
\includegraphics[scale=0.25]{initial_partition.png}
\caption{Initial partition of the graph where each $C_i$ is a sequence of columns}
\label{fig:init_part}
\end{figure}

Now each sequence of columns owned by a processor can be decomposed into square blocks. To compute the next iteration of the Floyd-Warshall algorithm for a single block, say block number $i$ in processor $2$, we need the $i^{th}$ block from all other processors.

\begin{figure}[H]
\centering
\includegraphics[scale=0.25]{allgather.png}
\caption{For block $R_{i1}$ we need the $i^{th}$ block from all other processors}
\label{fig:allgather}
\end{figure}

Therefore, we do an \texttt{MPI\_Allgather} operation which gathers the $i^{th}$ block from all the processors and recreates the $i^{th}$ row chunk in all processors. Now, we can update block $R_{ij}$ for all $j$ processors. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.25]{row_col.png}
\caption{Updating the $i^{th}$ square block by processor 1 using row chunk $i$}
\label{fig:row_col}
\end{figure}

The \texttt{MPI\_Allgather} is then repeated until all square blocks have completed a step in the Floyd-Warshall algorithm. Each processor individually checks whether an update was made to their sequence of columns. To complete the iteration, we perform a \texttt{MPI\_Allreduce} operation to check whether \textit{any} update was made across \textit{all} processors. If an update was made, we continue the iteration. Otherwise, all processors terminate and the solution is reached. \\

The MPI implementation can be found in \texttt{path-mpi.c}. Note that the MPI implementation requires that the number of nodes of the graph to be divisible by the number of processors launched. 

\subsubsection{Advantages}

The MPI approach improves upon a serial implementation of the algorithm due to its ability to compute updates to multiple regions of the graph in parallel. \\
 
The MPI approach makes each processor be responsible of a contiguous sequence of columns in the graph. To update a a square block in this sequence of columns, the processor needs to recreate only the corresponding sequence of rows. Therefore, if the width of the sequence of columns is $d$ and the side length of the graph is $n$, each processor only needs to hold $2nd$ information in memory instead of $n^2$. On larger graphs, this decrease in memory footprint will prevent thrashing since the space scales as $O(n)$ instead of $O(n^2)$ and will show improvements in performance over the OpenMP version. 

\subsubsection{Disadvantages}
On smaller graphs, the communication and synchronization overhead  of MPI may cause a decrease in performance.

\section{Analysis}

\subsection{Original Implementation}
\subsubsection{Profiling} \label{sec:prof_orig}

Profiling the original solution shows that the most CPU time goes into the square function and significant portion of that time is considered by VTune to be ideal. The next most expensive functions in terms of CPU time is the barrier and the reduction in OpenMP due to the high spin times. 

\lstinputlisting[basicstyle=\tiny\ttfamily]{./profiling/path_original.txt}

\subsubsection{Scaling Study} \label{sec:speedup_orig}

The speedup plots of the original solution shows linear improvement in performance but an exponential decrease in efficiency as shown in figure~\ref{fig:ws_orig}. This can be attributed to the increased overhead in synchronization and spin time.

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/strong_scaling_original.png}
\caption{Strong scaling study of the original solution}
\label{fig:ss_orig}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/weak_scaling_original.png}
\caption{Weak scaling study of the original solution}
\label{fig:ws_orig}
\end{figure}

The strong scaling study was performed on a graph with 2000 nodes. The weak scaling varies the number of threads but keeps the problem size per processor be 1000 nodes. 

\newpage
\subsection{Tuned Parallel Implementation}

\subsubsection{Profiling} \label{sec:prof}

We had two version of the tuned parallel code in \texttt{OpenMP}. The output from the first version is shown here:

\lstinputlisting[basicstyle=\tiny\ttfamily]{./profiling/profile_elliot.txt}

The output from the second version is shown here. Note that this was run with 16 threads.
\lstinputlisting[basicstyle=\tiny\ttfamily]{./profiling/profile_eric.txt}

\subsubsection{Scaling Study} \label{sec:speedup}

\paragraph{Tuned approach I}

Although the scaling studies for the tuned parallel implementation times show similar speedup and efficiency patterns as the original solution, the overall time taken to achieve the solution is lower than the original OpenMP solution. 
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/strong_scaling_elliot.png}
\caption{Strong scaling study of the tuned parallel solution}
\label{fig:ss_elliot}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/weak_scaling_elliot.png}
\caption{Weak scaling study of the tuned parallel solution}
\label{fig:ws_elliot}
\end{figure}

The strong scaling study was performed on a graph with 2048 nodes. The weak scaling varies the number of threads but keeps the problem size per processor be 512 nodes. \\

\paragraph{Tuned approach II}

We then attempted to explore a different means of parallelization in which threads were assigned to much larger blocks based on their thread id, which could be obtained using the \texttt{omp\_get\_thread\_num()} function. We obtained a significant speedup! For a boardsize of 2048 x 2048, our tuned implementation finished in 1.03249 seconds using 16 threads, whereas the naive finished in 11.0959, leading to a speedup factor of 11. \\

Looking at the vectorization report, the large majority of the computationally heavy calculations were being vectorized, resulting in a large speedup. We ran the strong scaling studying on a 2048 x 2048 graph. The weak scaling study varies the problem size but keeps the problem size per processor constant at a 512 x 512 board. We could only run it using a small number of threads: 1, 4, and 16 on the compute node.

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/strong_scaling_eric.png}
\caption{Strong scaling study of the tuned parallel solution}
\label{fig:ss_eric}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/weak_scaling_eric.png}
\caption{Weak scaling study of the tuned parallel solution}
\label{fig:ws_eric}
\end{figure}

\paragraph{Offloading of Tuned Approach II}
We then took our most tuned OpenMP approach, and ran it on the Xeon Phi coprocessors. Since the coprocessors have many more cores than the nodes, we hoped this would give us more data for scaling studies, and would be able to run the same problem faster overall given the higher theoretical peak flop rate. The weak study was done with a constant workload of a 200 x 200 graph per processor, while the strong scaling study was done with a 2000 x 2000 graph.The results can be found in the following figures:

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/strong_scaling_offload.png}
\caption{Strong scaling study of the tuned parallel solution on the Xeon Phi coprocessor}
\label{fig:ss_offload}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/weak_scaling_offload.png}
\caption{Weak scaling study of the tuned parallel solution on the Xeon Phi coprocessor}
\label{fig:ws_offload}
\end{figure}

While our strong scaling study is surprisingly not always monotone, we do see that we are able to get a maximum speedup of almost 20 over the serial implementation.

\subsection{MPI Implementation}

\subsubsection{Profiling}

Profiling of the MPI implementation running on a graph with 1000 nodes with 10 processors shows that the bottleneck of the implementation is in the \texttt{mult} function which performs the row column 'squaring' operation. The spin times correspond to the blocking operations of MPI required to synchronize graph state across processes. 

\lstinputlisting[basicstyle=\tiny\ttfamily]{./profiling/profile_mpi.txt}


\subsubsection{Scaling Study}

The strong scaling study for the MPI implementation was done on a graph with 1000 nodes. The speedup plot (figure~\ref{fig:ss_mpi}) shows good speedup over the original SMP code but seems to plateau with increasing number of threads. We believe that at higher number of processors, the amount of communication between processors to carry out one iteration of updating overwhelms the benefits of parallelism. Note that the speedup plots do not include time to initially split up the work. The reasoning behind the decision to exclude the time taken to spread out the work was that applications could be coded so that each processor initializes its own thread independently. \texttt{path\_mpi.c} we had to distribute the work to have the same hash as the OpenMP version. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/strong_scaling_mpi.png}
\caption{Strong scaling study of the MPI implementation}
\label{fig:ss_mpi}
\end{figure}

The weak scaling study in figure~\ref{fig:ws_mpi} shows an exponential decrease in efficiency as we increase the number of processors. This is understandable due to the increased communication required by the \texttt{MPI\_Allgather} operations and \texttt{MPI\_Allreduce} operations as the number of processes increase. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/weak_scaling_mpi.png}
\caption{Weak scaling study of the MPI implementation}
\label{fig:ws_mpi}
\end{figure}


\begin{thebibliography}{9}
\bibitem{writeup}
Bindel, D. \textit{All-Pairs Shortest Paths}. Retrieved November 10, 2015, from \url{https://github.com/sheroze1123/path/blob/master/main.pdf}

\bibitem{cannon}
Hyuk-Jae Lee, James P. Robertson, and Josï¿½ A. B. Fortes. 1997. \textit{Generalized Cannon's algorithm for parallel matrix multiplication}. In Proceedings of the 11th international conference on Supercomputing (ICS '97). ACM, New York, NY, USA, 44-51. DOI=http://dx.doi.org/10.1145/263580.263591

\end{thebibliography}

 
 
\end{document}
