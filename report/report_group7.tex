
\documentclass[11pt]{article}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{float}
\usepackage{inconsolata}
\usepackage{url}
\usepackage{titlesec}
\setcounter{secnumdepth}{4}




\graphicspath{{./times/}}
\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.5cm
\textwidth16.5cm
\footskip1.0cm

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}


\begin{document}
\title{CS 5220\\ Project 3 - All-Pairs Shortest Path}
\author{Eric Gao (emg222)\\ Elliot Cartee (evc34)\\ Sheroze Sheriffdeen(mss385)}
\maketitle

\section{Introduction}
The Floyd-Warshall algorithm computes the pair-wise shortest path lengths given a graph with a metric. The computational pattern of this algorithm is very much akin to matrix multiplication. If $l_{ij}^s$ represents the the length of the shortest path from node $i$ to $j$ in at most $2^s$ steps, then
\begin{equation}
	l_{ij}^{s+1} = \min_k \{ l_{ik}^s + l_{kj}^s \}
\end{equation} 
 represents the shortest path from $i$ to $j$ of at most $2^{j+1}$ hops. \cite{writeup} 
\section{Design Decisions}
 
\subsection{Parallel Tuning}

Since the Floyd-Warshall algorithm is structured very similarly to matrix multiplication, we decided to try taking some of the tuning methods used in the first project on matrix multiplication, and applying them to the Floyd-Warshall algorithm.

The first of these methods is using a blocking scheme, so that updating the shortest path lengths is done through repeated calls to a small kernel. Since the size of this small kernel is known at compile-time, the compiler is able to optimize this small kernel extremely efficiently. Currently our implementation of blocking only works when the number of vertices is divisible by the block size, but we plan to correct this soon.

The second method was to change the loop ordering, so that in the innermost loop, memory is being accessed with unit stride, which increases performance.

As the project goes on, we also hope to add other optimizations attempted in the matrix multiplication project, such as copying the blocks into a contiguous chunk of memory.

\subsection{Message Passing Interface}\label{sec:mpi}

In addition to the tuned parallel implementations, we explored an approach that uses the Message Passing Interface to achieve parallelism. In the MPI implementation, each process handles a certain region of the graph. To prevent a master process orchestrating the distance computation, we ideally want each process to only wait for information from the relevant part of the graph. To that end, we take the adjacency matrix on which the Floyd-Warshall algorithm is run and partition the graph by chunks of columns. Then each processors is responsible for update a contiguous sequence of columns in the matrix.

\begin{figure}[H]
\centering
\includegraphics[scale=0.25]{initial_partition.png}
\caption{Initial partition of the graph where each $C_i$ is a sequence of columns}
\label{fig:init_part}
\end{figure}

Now each sequence of columns owned by a processor can be decomposed into square blocks. To compute the next iteration of the Floyd-Warshall algorithm for a single block, say block number $i$ in processor $2$, we need the $i^{th}$ block from all other processors.

\begin{figure}[H]
\centering
\includegraphics[scale=0.25]{allgather.png}
\caption{For block $R_{i1}$ we need the $i^{th}$ block from all other processors}
\label{fig:allgather}
\end{figure}

Therefore, we do an \texttt{MPI\_Allgather} operation which gathers the $i^{th}$ block from all the processors and recreates the $i^{th}$ row chunk in all processors. Now, we can update block $R_{ij}$ for all $j$ processors. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.25]{row_col.png}
\caption{Updating the $i^{th}$ square block by processor 1 using row chunk $i$}
\label{fig:row_col}
\end{figure}

The \texttt{MPI\_Allgather} is then repeated until all square blocks have completed a step in the Floyd-Warshall algorithm. Each processor individually checks whether an update was made to their sequence of columns. To complete the iteration, we perform a \texttt{MPI\_Allreduce} operation to check whether any update was made across all processors. If an update was made, we continue the iteration. Otherwise, all processors terminate and the solution is reached.

\subsubsection{Advantages}

The MPI approach improves upon a serial implementation of the algorithm due to its ability to compute updates to multiple regions of the graph in parallel.
 
The MPI approach makes each processor be responsible of a contiguous sequence of columns in the graph. To update a a square block in this sequence of columns, the processor needs to recreate only the corresponding sequence of rows. Therefore, if the width of the sequence of columns is $d$ and the side length of the graph is $n$, each processor only needs to hold $2nd$ information in memory instead of $n^2$. On larger graphs, this decrease in memory footprint will prevent thrashing since the space scales as $O(n)$ instead of $O(n^2)$ and will show improvements in performance over the OpenMP version.

\subsubsection{Disadvantages}
On smaller graphs, the communication and synchronization overhead  of MPI may cause a decrease in performance.

\subsubsection{Implementation}
The MPI implementation can be found in \texttt{path-mpi.c}. We are still in the process of ironing out bugs in the implementation and hope to have a complete solution by the final report. Furthermore, if time permits, we aim to explore the Cannon's algorithm to improve upon the current MPI scheme. \cite{cannon}

\section{Analysis}

\subsection{Original Implementation}
\subsubsection{Profiling} \label{sec:prof_orig}

Profiling the original solution shows that the most CPU time goes into the square function and significant portion of that time is considered by VTune to be ideal. The next most expensive functions in terms of CPU time is the barrier and the reduction in OpenMP due to the high spin times. 

\lstinputlisting[basicstyle=\tiny\ttfamily]{./profiling/path_original.txt}

\subsubsection{Scaling Study} \label{sec:speedup_orig}

The speedup plots of the original solution shows linear improvement in performance but an exponential decrease in efficiency as shown in figure~\ref{fig:ws_orig}. This can be attributed to the increased overhead in synchronization and spin time.

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/strong_scaling_original.png}
\caption{Strong scaling study of the original solution}
\label{fig:ss_orig}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/weak_scaling_original.png}
\caption{Weak scaling study of the original solution}
\label{fig:ws_orig}
\end{figure}

The strong scaling study was performed on a graph with 2000 nodes. The weak scaling varies the number of threads but keeps the problem size per processor be 1000 nodes. 

\newpage
\subsection{Tuned Parallel Implementation}

\subsubsection{Profiling} \label{sec:prof}

We had two version of the tuned parallel code in \texttt{OpenMP}. The output from the first version is shown here:

\lstinputlisting[basicstyle=\tiny\ttfamily]{./profiling/profile_elliot.txt}

The output from the second version is shown here. Note that this was run with 16 threads.
\lstinputlisting[basicstyle=\tiny\ttfamily]{./profiling/profile_eric.txt}

\subsubsection{Scaling Study} \label{sec:speedup}

Although the scaling studies for the tuned parallel implementation times show similar speedup and efficiency patterns as the original solution, the overall time taken to achieve the solution is lower than the original OpenMP solution. 
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/strong_scaling_elliot.png}
\caption{Strong scaling study of the tuned parallel solution}
\label{fig:ss_elliot}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/weak_scaling_elliot.png}
\caption{Weak scaling study of the tuned parallel solution}
\label{fig:ws_elliot}
\end{figure}

The strong scaling study was performed on a graph with 2048 nodes. The weak scaling varies the number of threads but keeps the problem size per processor be 512 nodes. 

We then attempted to explore a different means of parallelization in which threads were assigned to much larger blocks based on their thread id, which could be obtained using the \texttt{omp\_get\_thread\_num()} function. We obtained a significant speedup! For a boardsize of 2048 x 2048, our tuned implementation finished in 1.03249 seconds using 16 threads, wheras the naive finished in 11.0959, which means we had roughly \textbf{11 times speedup}! 

Looking at the vectorization report, the large majority of the computationally heavy calculations were being vectorized, resulting in a large speedup. However, we have not yet generalized this code and this can only run for some fixed problem sizes and fixed thread sizes. We plan to do this in the future. Therefore, we ran the strong scaling studying on a 2048 x 2048 graph. The weak scaling study varies the problem size but keeps the problem size per processor constant at a 512 x 512 board. We could only run it using a small number of threads: 1, 4, and 16 on the compute node.

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/strong_scaling_eric.png}
\caption{Strong scaling study of the tuned parallel solution}
\label{fig:ss_eric}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{./scaling_studies/weak_scaling_eric.png}
\caption{Weak scaling study of the tuned parallel solution}
\label{fig:ws_eric}
\end{figure}

\section{Future Work}

In addition to completing the MPI implementation outlined in Section~\ref{sec:mpi}, we aim to explore the Cannon's algorithm \cite{cannon} to further decrease the memory footprint in each processor and improve parallelization. 

\begin{thebibliography}{9}
\bibitem{writeup}
Bindel, D. \textit{All-Pairs Shortest Paths}. Retrieved November 10, 2015, from \url{https://github.com/sheroze1123/path/blob/master/main.pdf}

\bibitem{cannon}
Hyuk-Jae Lee, James P. Robertson, and José A. B. Fortes. 1997. \textit{Generalized Cannon's algorithm for parallel matrix multiplication}. In Proceedings of the 11th international conference on Supercomputing (ICS '97). ACM, New York, NY, USA, 44-51. DOI=http://dx.doi.org/10.1145/263580.263591

\end{thebibliography}

 
 
\end{document}
