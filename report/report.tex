\documentclass[11pt]{article}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{float}
\usepackage{url}
\usepackage{titlesec}
\setcounter{secnumdepth}{4}


\graphicspath{{./times/}}
\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.0cm
\textwidth16.5cm
\footskip1.0cm

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}


\begin{document}
\title{CS 5220\\ Project 3 - All Pairs Shortest Path}
\author{Marc Aurele Gilles (mtg79)\\ Edward Tremel (ejt64) \\ Yu Su (ys576)}
\maketitle

\section{Introduction}
In this project the goal is to profile, parallelize and tune an algorithm which computes the shortest path between all the pairs in a graph.


\section{Profiling of Original Code}
The profiling was done using amplxe. By looking at the hotspots report, we observe that most of the time (45 out of 67 seconds) is spent in the \texttt{square} function, which is to be expected as all of the computation is done in this function. A notable point, however, is that virtually all the rest of the time is spent waiting, which seems to indicate a significant load imbalance between the threads.
Looking at the time spent inside of the \texttt{square} function, we observe that a majority of the time (27 out of 45 seconds) is spent fetching memory and writing it to a float, which could indicate that there is poor cache utilization.

This profiling leads us to believe that we should try to address two problems for the OMP implementation: have better load balancing, and increase cache reuse.



\section{Initial MPI Implementation}
For our MPI implementation, we assign each processor to a large block of the matrix instead of an entire column as the original code does.
Note that to update one entry of the matrix for one step of the outer loop we need to read the whole column and row of that entry. 
Therefore, if each processor is assigned to a column, each processor needs to access each other column, and thus needs to communicate with all of the other processors. 
By assigning each processor to a block of the matrix instead, we allow each processor to have to communicate with only the blocks that share its row and its column.
We believe this will alleviate the load balancing problem seen in the OpenMP code.

To avoid global communications with this blocking strategy, we create $2n_{block}$ groups and communicators, where $n_{block}=n/blocksize$. 
Here $n$ is the dimension of the matrix and $blocksize$ is the dimension of the blocks. 
That is, we have one communicator for each row of blocks, and one communicator for each column of blocks. 
Each thread, which owns its own block, is then added to the communicator of its row and to the communicator of its column. 
Therefore each thread is in two different communicators (in addition to the world communicator), but no two threads are in the same two communicators.
To update its block, each processor makes one \textbf{\texttt{MPI\_Allgather}} call in its column communicator and one \textbf{ \texttt{MPI\_Allgather}} call in its row communicator, which reads all of the entries in its block column and block row, performs the update on its own block, and calls \textbf{\texttt{MPI\_Allgather}} again to share its result with the other processors.

The number of entries to be shared in this manner is $2 \cdot n \cdot blocksize$ per thread and per time step, compared to $n^2$ if each thread owned a column instead.

\section{Performance of Initial MPI Implementation}
We measured the runtime of our MPI implementation for various numbers of processors and compared it to the original OpenMP code running with various numbers of processors.
Unfortunately, since our MPI implementation cannot handle non-square numbers of processors, we were only able to test it with 1, 4, and 16 processors (a maximum of 24 processors are available on the Xeon main boards).
Figure \ref{fig:mpi-omp} shows the results for numbers of processors greater than 1.
The serial versions of both programs were so slow that we plotted them on a separate graph, figure \ref{fig:serial}.

\begin{figure}[h]
	\centering
	\includegraphics[width=.8\textwidth]{mpi_omp_comparison.pdf}
	\caption{Speed of the OpenMP and MPI implementations as the problem size and number of processors changes.}
	\label{fig:mpi-omp}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=.8\textwidth]{serial.pdf}
	\caption{Speed of the OpenMP and MPI implementations with only one processor.}
	\label{fig:serial}
\end{figure}

With a small number of processors (4), our MPI implementation runs about as well as the OpenMP code for small values of $n$, and scales much better for large values of $n$.
With 16 processors, our MPI implementation is actually a little slower than the OpenMP code at small values, and for some reason much slower for values of $n$ between 1800 and 2200.
However, for $n = 1600$ and also for large values of $n$, we again perform faster than OpenMP.


\section{Tuning the MPI Implementation}
After profiling the MPI implementation, we observed that most of the time was spent doing memory accesses. Therefore, we tried to optimize our MPI implementation by increasing the cache hits. 
Similarly to how we tuned matrix multiplication, we used kernel which uses a blocking strategy (on top of the blocking which is used for the parallelization), and copy optimization. 
Furthermore, we used aligned memory allocations to ease vectorization. 


We can increase cache utilization by subdividing each block into smaller blocks chosen to fill a thread's local cache.
The idea is similar to the matrix multiplication problem since they have a similar formulation.
With the Xeon E5-2620 v3, each core has 256KB L2 cache, so we choose a block size of 128. 
We apply copy optimization by using \texttt{\_\_mm\_\_malloc} to allocate buffers with a byte alignment of 64 bits. 
The reason that this byte alignment gives the best performance may be because the size of an integer in a 64-bit core is 64 bits. 

We tested our optimized block square function in a single thread and found that it runs in 80s with n=3072, compared with 350s spent in the original square function in a single thread. 
Thus the second level of blocking improves performance significantly even without parallelization. 
However, directly applying \texttt{\#pragma parallel for} to the sub-blocked version fails, so we only applied the optimized kernel to the MPI version.

\section{Performance Analysis of Tuned MPI}
To get an idea of how much our optimized kernal improved the MPI implementation, we tested its runtime on various graph sizes and compared it to our original MPI implementation and the OpenMP implementation.
We ran all three implementations with 16 threads, the largest number our MPI implementation can support due to its assumption that the number of threads is square.
Figure \ref{fig:size-comparison} shows that the optimized kernel gives us modest improvements at small problem sizes, but significant improvements at large problem sizes, especially over the OpenMP implementation which seems to suffer a catastrophic breakdown at problem sizes over n=4000.

\begin{figure*}[ht]
	\centering
	\begin{subfigure}[b]{.45\textwidth}
		\includegraphics[width=\textwidth]{comparison_small.pdf}
		\caption{Small problem sizes}
	\end{subfigure}
	\begin{subfigure}[b]{.45\textwidth}
		\includegraphics[width=\textwidth]{comparison_large.pdf}
		\caption{Large problem sizes}
	\end{subfigure}
	\caption{Speed of the three implementations as the problem size increases; block-mpi is our tuned MPI implementation}
	\label{fig:size-comparison}
\end{figure*}


\subsection{Strong Scaling Study}

\subsection{Weak Scaling Study}

\end{document}
